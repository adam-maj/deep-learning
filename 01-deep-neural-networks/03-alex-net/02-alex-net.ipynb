{"cells":[{"cell_type":"markdown","metadata":{"id":"Dj-er-0mTi5t"},"source":["# Explanation\n","\n","The creation of AlexNet was a milestone in the field of deep learning, completely transforming the field permanently.\n","\n","In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton presented AlexNet at the ILSVRC image classification competition and crushed the previous state-of-the-art model, bringing down the best top-5 error rate from 25.8% to 16.4%.\n","\n","AlexNet made significant innovations on almost all of the constraints I mentioned in the README of this project. It introduced new approaches in scaling parameters & model depth, improving optimization & regularization, introducing architectural innovations, and using more compute efficiently.\n","\n","Even more importantly than the technical innovations it introduced, it brought a new wave of focus on deep learning, revitalizing academic interest and triggering a series of progressions that led to GoogLeNet, ResNet, and many other continued innovations in the field.\n","\n","### Innovations\n","\n","AlexNet is a result of a combination of important innovations that created their state-of-the-art model.\n","\n","First, they introduced an architecture with a new level of depth and parameters that hadn't been seen before - they created a network with 5 convolutional layers and 3 feed forward layers, much larger than any previous CNNs. This increase in the number of parameters in the network greatly increased its capacity to learn useful representations - and it was enabled by a corresponding improvement in optimization & regularization strategies and compute.\n","\n","AlexNet was among the first networks to use the ReLU activation function extensively throughout the network, which contributed to increased compute efficiency (due to the efficiency of the gating activations of ReLU), and more importantly, enabled the network to form sparse representations more conducive to learning effectively.\n","\n","Additionally, AlexNet was an early user of dropout, demonstrating its efficacy in preventing overfitting and promoting generalization.\n","\n","They also introduced an architectural advancement to the CNN with the usage of overlapping pooling, in contrast with previous pooling strategies, which helped reduce the size of the network and further improved generalization.\n","\n","Finally, they were among the first to actually use GPUs in training their models to make the training process far faster. They innovated on this front in two ways.\n","\n","First, they used two GPUs in combination, and took advantage of the newly introduced ability for NVIDIA GPUs to write to each others memory to improve training since they no longer needed GPUs to communicate through the host machine (which was much more inconvenient).\n","\n","Additionally, they wrote their own GPU kernels for convolutions which enabled their network to run efficiently on GPUs in the first place.\n","\n","This combination of advancements led AlexNet to completely change deep learning, and contributed significantly to put the field on the trajectory that got us to where we are today."]},{"cell_type":"markdown","metadata":{"id":"siUKgE0lTi5u"},"source":["# My Notes\n","\n","ðŸ“œ [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n","\n","> Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images.\n","\n","> Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.\n","\n","Efficient use of compute has always been critical to practically pushing the boundaries on deep learning effectively.\n","\n","> In the end, the networkâ€™s size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate.\n","\n","> All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.\n","\n","> ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazonâ€™s Mechanical Turk crowd-sourcing tool.\n","\n","### **Architecture**\n","\n","**1. ReLU**\n","\n","8 layers total - 5 convolutional layers, 3 fully-connected feed-forward layers\n","\n","> Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.\n","\n","ReLU trains way faster than tanh because of the simplicity of differentiating it, which actually meaningfully contributes to the constraints.\n","\n","Also importantly, it is a non-saturating nonlinearity - functions like sigmoid and tanh (saturating) compress inputs into [0,1], which means at large magnitudes, their gradient approaches 0. This causes the _vanishing gradient problem_. Meanwhile, ReLU never runs into this problem.\n","\n","**2. Multiple GPUs**\n","\n","> A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the network that can be trained on it. It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU. Therefore we spread the net across two GPUs.\n","\n","> Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one anotherâ€™s memory directly, without going through host machine memory.\n","\n","> The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers.\n","\n","Communication between GPUs is tuned to be sensible for the amount of available compute (they were only using 2 GPUs). This was one of the first times someone really used multiple GPUs for training.\n","\n","**3. Local Response Normalization**\n","\n","> ReLUs have the desirable property that they do not require input normalization to prevent them from saturating.\n","\n","Normalization is not a necessity to prevent saturation (whereas it is necessary with tanh and sigmoid because of their vanishing gradients at large values).\n","\n","However, normalization still provides advantages.\n","\n","> This sort of response normalization implements a form of lateral inhibition\n","> inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels.\n","\n","They implement **local response normalization**. This prevents many kernels from firing on the same pixel, forcing more prominent features to stand out, and weaker features to be de-emphasized.\n","\n","**4. Overlapping Pooling**\n","\n","> Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap.\n","\n","Instead, AlexNet uses overlapping pooling layers which improves regularization (less overfitting) and decreases loss.\n","\n","**5. Overall Architecture**\n","\n","> The first convolutional layer filters the 224Ã—224Ã—3 input image with 96 kernels of size 11Ã—11Ã—3 with a stride of 4 pixels.\n","\n","> The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 Ã— 5 Ã— 48.\n","\n","> The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers.\n","\n","> The third convolutional layer has 384 kernels of size 3 Ã— 3 Ã— 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 Ã— 3 Ã— 192 , and the fifth convolutional layer has 256 kernels of size 3 Ã— 3 Ã— 192.\n","\n","### Regularization\n","\n","**1. Dataset Augmentation**\n","\n","> The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations\n","\n","They train the model on 224 x 224 chunks of the 256 x 256 pixel images, and augment the intensities of the RGB channels to regularize.\n","\n","**2. Dropout**\n","\n","> Combining the predictions of many different models is a very successful way to reduce test errors, but it appears to be too expensive for big neural networks that already take several days to train.\n","\n","We already know here the intuition behind multi-headed attention! Having multiple separate models run and develop their own intuitions, then combining their outputs is great where you can afford to do it.\n","\n","> There is, however, a very efficient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called â€œdropoutâ€, consists of setting to zero the output of each hidden neuron with probability 0.5.\n","\n","> This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons.\n","\n","Neurons canâ€™t co-adapt with each other by becoming highly interdependent on other neurons to process (higher dimensional) features.\n","\n","> It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n","\n","Instead, dropout compels individual neurons to learn more broadly useful features that can be useful with any group of neurons.\n","\n","> At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.\n","\n","Since the network was trained with 50% of the neurons active at one time, we expect to have 2x the total contributions at test time since all neurons are active, so we scale the outputs by 0.5.\n","\n","> Without dropout, our network exhibits substantial overfitting. Dropout roughly doubles the number of iterations required to converge.\n","\n","### Results & Discussion\n","\n","> Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%. The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2%.\n","\n","Their model beats state of the art by far.\n","\n","> The kernels on GPU 1 are largely color-agnostic, while the kernels\n","> on on GPU 2 are largely color-specific.\n","\n","Each GPU (part of the network on each GPU, which you actually need to think about) tends to specialize to different types of kernels as they are isolated.\n","\n","![Screenshot 2024-05-09 at 10.21.53â€¯AM.png](../../images/Screenshot_2024-05-09_at_10.21.53_AM.png)\n","\n","5 images in the training set with the 6 images closest to them based on the euclidian distance of the activations in the last layer (showing that later layers have combined data to form similarity calls between images).\n","\n","> Our results show that a large, deep convolutional neural network is capable of achieving record-breaking results on a highly challenging dataset using purely supervised learning.\n","\n","> It is notable that our networkâ€™s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.\n","\n","Depth is critical for the network to work effectively.\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}