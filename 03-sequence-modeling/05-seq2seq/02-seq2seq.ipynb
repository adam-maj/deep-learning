{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9aqV77CKOZUpVvtapje0X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Explanation\n","\n","This paper sets the foundation for the _Attention Is All You Need_ paper that introduced the transformer.\n","\n","It builds on the equivalent paper that introduces the Encoder-Decoder architecture for RNNs. This paper introduces the Seq2Seq model which is an Encoder-Decoder architecture built with LSTMs for machine translation.\n","\n","In terms of architecture, this paper almost entirely builds on previous work. The LSTM is still using the memory cells with forget gates we've covered.\n","\n","The Seq2Seq model uses 2 different LSTMs - one as the encoder to map text in an input sequence (often text in the language to be translated) into a low dimensional representation space, and another to map text in that representation space back up into a translation.\n","\n","Most things are similar to the RNN Encoder-Decoder architecture that came before this. The main difference is they feed words in the input sequence to the network in reverse order. Emprically, this seems to work because it puts initial words in the output sequence (the first words to translate) temporally close to the words in the input sequence.\n","\n","This paper showed how effective sequence modeling could be for the machine translation task, which set the grounds for the Transformer.\n","\n","Another important results of the Encoder-Decoder architecture noted here is that the constraint of fitting the encodings into a representation space creates a constraint conducive for meaningful embeddings as the output of the encoder."],"metadata":{"id":"575bYpbmYZr1"}},{"cell_type":"markdown","source":["# My Notes\n","\n","ðŸ“œ [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)\n","\n","> In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.\n","\n","> Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.\n","\n","This uses the encoder-decoder architecture with LSTMs for sequence-to-sequence tasks.\n","\n","> Given that translations tend to be paraphrases of the source sentences, the translation objective encourages the LSTM to find sentence representations that capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model is aware of word order and is fairly invariant to the active and passive voice.\n","\n","### The Model\n","\n","The three main changes from the default LSTM formulation.\n","\n","> First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously.\n","\n","> Second, we found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers.\n","\n","> Third, we found it extremely valuable to reverse the order of the words of the input sentence.\n","\n","### Experiments\n","\n","**1. Dataset Details**\n","\n","> We used the WMTâ€™14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words.\n","\n","**2. Decoding and Rescoring**\n","\n","> Once training is complete, we produce translation by finding the most likely translation according to the LSTM\n","\n","$$\n","\\hat{T} = \\arg \\max_T p(T|S)\n","$$\n","\n","> We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number $B$ of partial hypotheses, where a partial hypothesis is a prefix of some translation.\n","\n","> As soon as the â€œ<EOS>â€ symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses.\n","\n","Beam search is used to traverse the tree of sequence predictions to create a number of hypotheses, and then the available hypotheses that get created can get scored and selected from.\n","\n","**3. Reversing the Source Sentences**\n","\n","> We discovered that the LSTM learns much better when the source sentences are reversed.\n","\n","> While we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset.\n","\n","As with all attempts at interpretability in deep learning research, there are results that are empirically definitive, but the explanations are not clear. Attempts at rationalization are made post-hoc.\n","\n","> Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding. word in the target sentence. As a result, the problem has a large â€œminimal time lag.â€\n","\n","By reversing the sentence order, this â€œminimal time lagâ€ is reduced, which could impact backpropagation.\n","\n","> Backpropagation has an easier time â€œestablishing communicationâ€ between the source sentence and the target sentence, which in turn results in substantially improved overall performance.\n","\n","**6. Experimental Results**\n","\n","> While the decoded translations of the LSTM ensemble do not outperform the best WMTâ€™14 system, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT task by a sizeable margin, despite its inability to handle out-of-vocabulary words.\n","\n","![Screenshot 2024-05-15 at 5.25.40â€¯PM.png](../../images/Screenshot_2024-05-15_at_5.25.40_PM.png)\n","\n","**8. Model Analysis**\n","\n","> One of the attractive features of our model is its ability to turn a sequence of words into a vector of fixed dimensionality.\n","\n","![Screenshot 2024-05-15 at 5.21.38â€¯PM.png](../../images/Screenshot_2024-05-15_at_5.21.38_PM.png)\n","\n","![Screenshot 2024-05-15 at 5.22.18â€¯PM.png](../../images/Screenshot_2024-05-15_at_5.22.18_PM.png)\n","\n","### Conclusion\n","\n","> In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task.\n","\n","> We were surprised by the extent of the improvement obtained by reversing the words in the source sentences. We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler.\n","\n","> We were also surprised by the ability of the LSTM to correctly translate very long sentences.\n","\n","> Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform an SMT system, so further work will likely lead to even greater translation accuracies.\n"],"metadata":{"id":"rnbDWoaXsgI0"}}]}