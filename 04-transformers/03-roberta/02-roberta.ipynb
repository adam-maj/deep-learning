{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "RoBERTa builds directly on BERT as it's a replication study. Most importantly, it shows that BERT was undertrained and underoptimized. With the effort of RoBERTa, they managed to achieve state of the art results again just by replicating BERT.\n",
    "\n",
    "I find RoBERTa interesting because in my eyes, it's an early indication of the begining of the scaling laws starting to become true in a big way for the LLM wave. It shows that without any change in architecture, but just with more data and tuning of the architecture, we can achieve significantly better results.\n",
    "\n",
    "This trend continues with the later GPTs, and OpenAI's core bet becomes scaling laws. And they turn out to be right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  My Notes\n",
    "\n",
    "ðŸ“œ [RoBERTa: A Robustly Optimized BERT Pre-training Approach](https://arxiv.org/pdf/1907.11692)\n",
    "\n",
    "> We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.\n",
    "\n",
    "> These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.\n",
    "\n",
    "> Our modifications are simple, they include:\n",
    "> (1) training the model longer, with bigger batches, over more data\n",
    "> (2) removing the next sentence prediction objective\n",
    "> (3) training on longer sequences\n",
    "> (4) dynamically changing the masking pattern applied to the training data\n",
    "\n",
    "> Our training improvements show that masked language model pre-training, under the right design choices, is competitive with all other recently published methods.\n",
    "\n",
    "RoBERTa is about showing that the BERT architecture is actually capable of achieving state-of-the-art results, and questioning itâ€™s design choices.\n",
    "\n",
    "### Training Procedure Analysis\n",
    "\n",
    "**1. Static vs. Dynamic Masking**\n",
    "\n",
    "BERT uses static token masking where the masks are determined in advance. Instead, RoBERTa tries dynamic token masking which leads to slight improvements.\n",
    "\n",
    "**2. Model Input Format and Next Sentence Prediction**\n",
    "\n",
    "BERT uses next sentence prediction. RoBERTa finds that you can actually do better by eliminating this and just training on sequences of sentences from a single document.\n",
    "\n",
    "**3. Training with Larger Batches**\n",
    "\n",
    "RoBERTa uses a larger mini-batch size for training.\n",
    "\n",
    "**4. Text Encoding**\n",
    "\n",
    "> Using bytes [instead of unicode characters] makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any â€œunknownâ€ tokens.\n",
    "\n",
    "> Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degradation in performance and use this encoding in\n",
    "> the remainder of our experiments.\n",
    "\n",
    "### RoBERTa\n",
    "\n",
    "> Specifically, RoBERTa is trained with dynamic masking, FULL SENTENCES without NSP loss, large mini-batches and a larger byte-level BPE.\n",
    "\n",
    "> Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pre-training, and (2) the number of training passes through the data.\n",
    "\n",
    "![Screenshot 2024-05-16 at 10.57.50â€¯AM.png](../../images/Screenshot_2024-05-16_at_10.57.50_AM.png)\n",
    "\n",
    "> Crucially, RoBERTa uses the same masked language modeling pre-training objective and architecture as $\\textrm{BERT}_{\\textrm{LARGE}}$, yet consistently outperforms both $\\textrm{BERT}_{\\textrm{LARGE}}$ and $\\textrm{XLNet}_{\\textrm{LARGE}}$.\n",
    "\n",
    "> This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.\n",
    "\n",
    "![Screenshot 2024-05-16 at 10.59.51â€¯AM.png](../../images/Screenshot_2024-05-16_at_10.59.51_AM.png)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> These results illustrate the importance of these previously overlooked design decisions and suggest that BERTâ€™s pre-training objective remains competitive with recently proposed alternatives.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
